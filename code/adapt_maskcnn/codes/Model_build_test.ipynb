{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import keras.layers as KL\n",
    "import utils\n",
    "import visualize\n",
    "from visualize import display_images\n",
    "import model as modellib\n",
    "from model import *\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "model_dir = os.path.join(root_dir, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "dehaze_model_path = os.path.join(root_dir, \"mask_rcnn_domain.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine import Layer\n",
    "def reverse_gradient(X, hp_lambda):\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        return [tf.negative(grad) * hp_lambda]\n",
    "\n",
    "    g = K.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "\n",
    "    return y\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return reverse_gradient(x, self.hp_lambda)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'hp_lambda': self.hp_lambda}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Concatenate, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(28,28,1))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "C1 = x = Conv2D(256, (2,2))(inputs)\n",
    "C2 = x = Conv2D(128, (2,2))(x)\n",
    "C1 = Conv2D(128, (1,1))(C1)\n",
    "C2 = Conv2D(128, (1,1))(C2)\n",
    "C1_before = Flatten()(C1)\n",
    "C2 = Flatten()(C2)\n",
    "Flip = GradientReversal(0.1)\n",
    "C1_after = Flip(C1_before)\n",
    "C1 = Dense(128, activation='relu')(C1_after)\n",
    "C2 = Dense(128, activation='relu')(C2)\n",
    "\n",
    "C3 = Concatenate()([C1, C2])\n",
    "\n",
    "predictions = Dense(10, activation='softmax')(C3)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MS COCO Dataset\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "COCO_DIR = \"COCO\"  # TODO: enter value here\n",
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    BATCH_SIZE=2\n",
    "\n",
    "config = InferenceConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def domain_loss_graph(domain_label, domain_logits):\n",
    "    loss = K.categorical_crossentropy(domain_label, domain_logits)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_by_id(img_idx, img_dir, img_names):\n",
    "    image_name = img_names[img_idx]\n",
    "    image = skimage.io.imread(os.path.join(img_dir, image_name))\n",
    "    if image.shape[-1] == 4:\n",
    "        image = image[:,:,0:3]\n",
    "    image, window, scale, padding = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        padding=config.IMAGE_PADDING)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def domain_data_generator(dataset, config, domain_image_dir, domain_image_names, shuffle=True, augment=True, random_rois=0,\n",
    "                   batch_size=2, detection_targets=False):\n",
    "    \"\"\"A generator that returns images and corresponding target class ids,\n",
    "    bounding box deltas, and masks.\n",
    "\n",
    "    dataset: The Dataset object to pick data from\n",
    "    config: The model config object\n",
    "    shuffle: If True, shuffles the samples before every epoch\n",
    "    augment: If True, applies image augmentation to images (currently only\n",
    "             horizontal flips are supported)\n",
    "    random_rois: If > 0 then generate proposals to be used to train the\n",
    "                 network classifier and mask heads. Useful if training\n",
    "                 the Mask RCNN part without the RPN.\n",
    "    batch_size: How many images to return in each call\n",
    "    detection_targets: If True, generate detection targets (class IDs, bbox\n",
    "        deltas, and masks). Typically for debugging or visualizations because\n",
    "        in trainig detection targets are generated by DetectionTargetLayer.\n",
    "\n",
    "    Returns a Python generator. Upon calling next() on it, the\n",
    "    generator returns two lists, inputs and outputs. The containtes\n",
    "    of the lists differs depending on the received arguments:\n",
    "    inputs list:\n",
    "    - images: [batch, H, W, C]\n",
    "    - image_meta: [batch, size of image meta]\n",
    "    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\n",
    "    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
    "    - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\n",
    "    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\n",
    "    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\n",
    "                are those of the image unless use_mini_mask is True, in which\n",
    "                case they are defined in MINI_MASK_SHAPE.\n",
    "    - domain_class: [batch, y]\n",
    "\n",
    "    outputs list: Usually empty in regular training. But if detection_targets\n",
    "        is True then the outputs list contains target class_ids, bbox deltas,\n",
    "        and masks.\n",
    "    \"\"\"\n",
    "    b = 0  # batch item index\n",
    "    image_index = -1\n",
    "    image_ids = np.copy(dataset.image_ids)\n",
    "    error_count = 0\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                             config.RPN_ANCHOR_RATIOS,\n",
    "                                             config.BACKBONE_SHAPES,\n",
    "                                             config.BACKBONE_STRIDES,\n",
    "                                             config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "    # Keras requires a generator to run indefinately.\n",
    "    while True:\n",
    "        try:\n",
    "            # Increment index to pick next image. Shuffle if at the start of an epoch.\n",
    "            image_index = (image_index + 1) % len(image_ids)\n",
    "            if shuffle and image_index == 0:\n",
    "                np.random.shuffle(image_ids)\n",
    "                np.random.shuffle(domain_image_names)\n",
    "            \n",
    "            # Get GT bounding boxes and masks for image.\n",
    "            image_id = image_ids[image_index]\n",
    "            image, image_meta, gt_class_ids, gt_boxes, gt_masks = \\\n",
    "                load_image_gt(dataset, config, image_id, augment=augment,\n",
    "                              use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "            \n",
    "            # Get the dehaze image\n",
    "            domain_image = get_image_by_id(image_index, domain_image_dir, domain_image_names)\n",
    "            domain_image = mold_image(domain_image.astype(np.float32), config)\n",
    "            \n",
    "            # Skip images that have no instances. This can happen in cases\n",
    "            # where we train on a subset of classes and the image doesn't\n",
    "            # have any of the classes we care about.\n",
    "            if not np.any(gt_class_ids > 0):\n",
    "                continue\n",
    "\n",
    "            # RPN Targets\n",
    "            rpn_match, rpn_bbox = build_rpn_targets(image.shape, anchors,\n",
    "                                                    gt_class_ids, gt_boxes, config)\n",
    "\n",
    "            # Mask R-CNN Targets\n",
    "            if random_rois:\n",
    "                rpn_rois = generate_random_rois(\n",
    "                    image.shape, random_rois, gt_class_ids, gt_boxes)\n",
    "                if detection_targets:\n",
    "                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\\\n",
    "                        build_detection_targets(\n",
    "                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, config)\n",
    "            # Init batch arrays\n",
    "            if b == 0:\n",
    "                batch_image_meta = np.zeros(\n",
    "                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype)\n",
    "                batch_rpn_match = np.zeros(\n",
    "                    [batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)\n",
    "                batch_rpn_bbox = np.zeros(\n",
    "                    [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)\n",
    "                batch_images = np.zeros(\n",
    "                    (batch_size,) + image.shape, dtype=np.float32)\n",
    "                batch_gt_class_ids = np.zeros(\n",
    "                    (batch_size, config.MAX_GT_INSTANCES), dtype=np.int32)\n",
    "                batch_gt_boxes = np.zeros(\n",
    "                    (batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32)\n",
    "                \n",
    "                # add batch domain class label\n",
    "                batch_domain_classes = np.zeros((batch_size, 2), dtype = np.float32)\n",
    "                \n",
    "                \n",
    "                if config.USE_MINI_MASK:\n",
    "                    batch_gt_masks = np.zeros((batch_size, config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1],\n",
    "                                               config.MAX_GT_INSTANCES))\n",
    "                else:\n",
    "                    batch_gt_masks = np.zeros(\n",
    "                        (batch_size, image.shape[0], image.shape[1], config.MAX_GT_INSTANCES))\n",
    "                if random_rois:\n",
    "                    batch_rpn_rois = np.zeros(\n",
    "                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)\n",
    "                    if detection_targets:\n",
    "                        batch_rois = np.zeros(\n",
    "                            (batch_size,) + rois.shape, dtype=rois.dtype)\n",
    "                        batch_mrcnn_class_ids = np.zeros(\n",
    "                            (batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)\n",
    "                        batch_mrcnn_bbox = np.zeros(\n",
    "                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)\n",
    "                        batch_mrcnn_mask = np.zeros(\n",
    "                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)\n",
    "\n",
    "            # If more instances than fits in the array, sub-sample from them.\n",
    "            if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:\n",
    "                ids = np.random.choice(\n",
    "                    np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)\n",
    "                gt_class_ids = gt_class_ids[ids]\n",
    "                gt_boxes = gt_boxes[ids]\n",
    "                gt_masks = gt_masks[:, :, ids]\n",
    "\n",
    "            # Add to batch\n",
    "            batch_image_meta[b] = image_meta\n",
    "            batch_rpn_match[b] = rpn_match[:, np.newaxis]\n",
    "            batch_rpn_bbox[b] = rpn_bbox\n",
    "            batch_images[b] = mold_image(image.astype(np.float32), config)\n",
    "            batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids\n",
    "            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes\n",
    "            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks\n",
    "            batch_domain_classes[b] = [1, 0]\n",
    "            \n",
    "            \n",
    "            # Domain index\n",
    "            d_index = batch_size/2 + b\n",
    "            batch_images[d_index] = domain_image\n",
    "            batch_domain_classes[d_index] = [0, 1]\n",
    "            \n",
    "            batch_image_meta[d_index] = image_meta\n",
    "            batch_rpn_match[d_index] = rpn_match[:, np.newaxis]\n",
    "            batch_rpn_bbox[d_index] = rpn_bbox\n",
    "            batch_gt_class_ids[d_index, :gt_class_ids.shape[0]] = gt_class_ids\n",
    "            batch_gt_boxes[d_index, :gt_boxes.shape[0]] = gt_boxes\n",
    "            batch_gt_masks[d_index, :, :, :gt_masks.shape[-1]] = gt_masks\n",
    "\n",
    "\n",
    "            if random_rois:\n",
    "                batch_rpn_rois[b] = rpn_rois\n",
    "                if detection_targets:\n",
    "                    batch_rois[b] = rois\n",
    "                    batch_mrcnn_class_ids[b] = mrcnn_class_ids\n",
    "                    batch_mrcnn_bbox[b] = mrcnn_bbox\n",
    "                    batch_mrcnn_mask[b] = mrcnn_mask\n",
    "            b += 1\n",
    "\n",
    "            # Batch full?\n",
    "            if 2*b >= batch_size:\n",
    "                inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,\n",
    "                          batch_gt_class_ids, batch_gt_boxes, batch_gt_masks, batch_domain_classes]\n",
    "                outputs = []\n",
    "\n",
    "                if random_rois:\n",
    "                    inputs.extend([batch_rpn_rois])\n",
    "                    if detection_targets:\n",
    "                        inputs.extend([batch_rois])\n",
    "                        # Keras requires that output and targets have the same number of dimensions\n",
    "                        batch_mrcnn_class_ids = np.expand_dims(\n",
    "                            batch_mrcnn_class_ids, -1)\n",
    "                        outputs.extend(\n",
    "                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])\n",
    "\n",
    "                yield inputs, outputs\n",
    "\n",
    "                # start a new batch\n",
    "                b = 0\n",
    "        except (GeneratorExit, KeyboardInterrupt):\n",
    "            raise\n",
    "        except:\n",
    "            # Log it and skip the image\n",
    "            logging.exception(\"Error processing image {}\".format(\n",
    "                dataset.image_info[image_id]))\n",
    "            error_count += 1\n",
    "            if error_count > 5:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_partial_image_meta_graph(meta):\n",
    "    \"\"\"Parses a tensor that contains image attributes to its components.\n",
    "    See compose_image_meta() for more details.\n",
    "\n",
    "    meta: [batch, meta length] where meta length depends on NUM_CLASSES\n",
    "    \"\"\"\n",
    "    image_id = meta[0:1, 0]\n",
    "    image_shape = meta[0:1, 1:4]\n",
    "    window = meta[0:1, 4:8]   # (y1, x1, y2, x2) window of image in in pixels\n",
    "    active_class_ids = meta[0:1, 8:]\n",
    "    return [image_id, image_shape, window, active_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first available GPU\n",
    "DEVICE_ID_LIST = GPUtil.getFirstAvailable()\n",
    "DEVICE_ID = DEVICE_ID_LIST[0] # grab first element from list\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Domain_MaskRCNN():\n",
    "    \"\"\"Encapsulates the Mask RCNN model functionality.\n",
    "\n",
    "    The actual Keras model is in the keras_model property.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode, config, model_dir):\n",
    "        \"\"\"\n",
    "        mode: Either \"training\" or \"inference\"\n",
    "        config: A Sub-class of the Config class\n",
    "        model_dir: Directory to save training logs and trained weights\n",
    "        \"\"\"\n",
    "        assert mode in ['training', 'inference']\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "        self.model_dir = model_dir\n",
    "        self.set_log_dir()\n",
    "        self.keras_model = self.build(mode=mode, config=config)\n",
    "        \n",
    "                    \n",
    "\n",
    "    def build(self, mode, config):\n",
    "        def layer_slice(x):\n",
    "            return x[0:1]\n",
    "        \"\"\"Build Mask R-CNN architecture.\n",
    "            input_shape: The shape of the input image.\n",
    "            mode: Either \"training\" or \"inference\". The inputs and\n",
    "                outputs of the model differ accordingly.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Since all other GPUs are masked out, the first available GPU will now be identified as GPU:0\n",
    "        device = '/gpu:0'\n",
    "        print('Device ID (masked): ' + str(0))\n",
    "\n",
    "        # Creates a graph.\n",
    "        with tf.device(device):\n",
    "\n",
    "            assert mode in ['training', 'inference']\n",
    "\n",
    "            # Image size must be dividable by 2 multiple times\n",
    "            h, w = config.IMAGE_SHAPE[:2]\n",
    "            if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "                raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                                \"to avoid fractions when downscaling and upscaling.\"\n",
    "                                \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "            # Inputs\n",
    "            input_image = KL.Input(\n",
    "                shape=config.IMAGE_SHAPE.tolist(), name=\"input_image\")\n",
    "            ori_input_image_meta = KL.Input(shape=[None], name=\"input_image_meta\")\n",
    "            if mode == \"training\":\n",
    "                \n",
    "                input_image_meta = KL.Lambda(layer_slice, name='sliced_image_meta')(ori_input_image_meta)\n",
    "                \n",
    "                \n",
    "                # RPN GT\n",
    "                ori_input_rpn_match = KL.Input(\n",
    "                    shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n",
    "                input_rpn_match = KL.Lambda(layer_slice, name='sliced_rpn_match')(ori_input_rpn_match)\n",
    "                \n",
    "                ori_input_rpn_bbox = KL.Input(\n",
    "                    shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
    "                input_rpn_bbox = KL.Lambda(layer_slice, name='sliced_rpn_bbox')(ori_input_rpn_bbox)\n",
    "                \n",
    "                \n",
    "                # Detection GT (class IDs, bounding boxes, and masks)\n",
    "                # 1. GT Class IDs (zero padded)\n",
    "                ori_input_gt_class_ids = KL.Input(\n",
    "                    shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n",
    "                input_gt_class_ids = KL.Lambda(layer_slice, name='sliced_gt_class_ids')(ori_input_gt_class_ids)\n",
    "                \n",
    "                # 2. GT Boxes in pixels (zero padded)\n",
    "                # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n",
    "                ori_input_gt_boxes = KL.Input(\n",
    "                    shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n",
    "                input_gt_boxes = KL.Lambda(layer_slice, name='sliced_gt_boxes')(ori_input_gt_boxes)\n",
    "                \n",
    "                # Normalize coordinates\n",
    "                h, w = K.shape(input_image)[1], K.shape(input_image)[2]\n",
    "                image_scale = K.cast(K.stack([h, w, h, w], axis=0), tf.float32)\n",
    "                gt_boxes = KL.Lambda(lambda x: x / image_scale)(input_gt_boxes)\n",
    "                # 3. GT Masks (zero padded)\n",
    "                # [batch, height, width, MAX_GT_INSTANCES]\n",
    "                if config.USE_MINI_MASK:\n",
    "                    ori_input_gt_masks = KL.Input(\n",
    "                        shape=[config.MINI_MASK_SHAPE[0],\n",
    "                               config.MINI_MASK_SHAPE[1], None],\n",
    "                        name=\"input_gt_masks\", dtype=bool)\n",
    "                else:\n",
    "                    ori_input_gt_masks = KL.Input(\n",
    "                        shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\n",
    "                        name=\"input_gt_masks\", dtype=bool)\n",
    "                \n",
    "                input_gt_masks = KL.Lambda(layer_slice, name='sliced_gt_masks')(ori_input_gt_masks)\n",
    "                \n",
    "                \n",
    "                # Domain class label\n",
    "                input_domain_label = KL.Input(shape=[2], name = \"input_domain_label\", dtype = tf.float32)\n",
    "\n",
    "            # Build the shared convolutional layers.\n",
    "            # Bottom-up Layers\n",
    "            # Returns a list of the last layers of each stage, 5 in total.\n",
    "            # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "            _, C2, C3, C4, C5 = resnet_graph(input_image, \"resnet101\", stage5=True)\n",
    "            # Top-down Layers\n",
    "            # TODO: add assert to varify feature map sizes match what's in config\n",
    "            P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)\n",
    "            P4 = KL.Add(name=\"fpn_p4add\")([\n",
    "                KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n",
    "                KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])\n",
    "            P3 = KL.Add(name=\"fpn_p3add\")([\n",
    "                KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n",
    "                KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])\n",
    "            P2 = KL.Add(name=\"fpn_p2add\")([\n",
    "                KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n",
    "                KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])\n",
    "            # Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "            P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
    "            P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
    "            P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
    "            P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n",
    "            # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "            # subsampling from P5 with stride of 2.\n",
    "            P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
    "\n",
    "            # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "            \n",
    "            \n",
    "            # Domain classification model\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            Flip = GradientReversal(0.1)\n",
    "            \n",
    "            F2 = Flip(P2)\n",
    "            F3 = Flip(P3)\n",
    "            F4 = Flip(P4)\n",
    "            F5 = Flip(P5)\n",
    "            \n",
    "            # F2 is 256*256, it goes through 4 2*2 conv2D with maxpolling and 1 1*1 conv\n",
    "            # F3 is 128*128, it goes through 3 2*2 conv2D with maxpolling and 2 1*1 conv\n",
    "            # F4 is 64*64, it goes through 2 2*2 conv2D with maxpolling and 3 1*1 conv\n",
    "            # F5 is 32*32, it goes through 1 2*2 conv2D with maxpolling and 4 1*1 conv\n",
    "            \n",
    "            \n",
    "            # 1st layer\n",
    "            F2 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_0_2_2_conv_f2\")(F2)\n",
    "            F3 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_0_1_1_conv_f3\")(F3)\n",
    "            F4 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_0_1_1_conv_f4\")(F4)\n",
    "            F5 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_0_1_1_conv_f5\")(F5)\n",
    "            \n",
    "            F2 = KL.Activation('relu')(F2)\n",
    "            F3 = KL.Activation('relu')(F3)\n",
    "            F4 = KL.Activation('relu')(F4)\n",
    "            F5 = KL.Activation('relu')(F5)\n",
    "            \n",
    "            \n",
    "            F2 = KL.MaxPool2D((2,2), name=\"domain_0_2_2_max_f2\")(F2)\n",
    "            \n",
    "            # 2nd layer\n",
    "            F2 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_1_2_2_conv_f2\")(F2)\n",
    "            F3 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_1_2_2_conv_f3\")(F3)\n",
    "            F4 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_1_1_1_conv_f4\")(F4)\n",
    "            F5 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_1_1_1_conv_f5\")(F5)\n",
    "            \n",
    "            F2 = KL.Activation('relu')(F2)\n",
    "            F3 = KL.Activation('relu')(F3)\n",
    "            F4 = KL.Activation('relu')(F4)\n",
    "            F5 = KL.Activation('relu')(F5)\n",
    "            \n",
    "            F2 = KL.MaxPool2D((2,2), name=\"domain_1_2_2_max_f2\")(F2)\n",
    "            F3 = KL.MaxPool2D((2,2), name=\"domain_1_2_2_max_f3\")(F3)\n",
    "            \n",
    "            # 3rd layer\n",
    "            F2 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_2_2_2_conv_f2\")(F2)\n",
    "            F3 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_2_2_2_conv_f3\")(F3)\n",
    "            F4 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_2_2_2_conv_f4\")(F4)\n",
    "            F5 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_2_1_1_conv_f5\")(F5)\n",
    "            \n",
    "            F2 = KL.Activation('relu')(F2)\n",
    "            F3 = KL.Activation('relu')(F3)\n",
    "            F4 = KL.Activation('relu')(F4)\n",
    "            F5 = KL.Activation('relu')(F5)\n",
    "            \n",
    "            F2 = KL.MaxPool2D((2,2), name=\"domain_2_2_2_max_f2\")(F2)\n",
    "            F3 = KL.MaxPool2D((2,2), name=\"domain_2_2_2_max_f3\")(F3)\n",
    "            F4 = KL.MaxPool2D((2,2), name=\"domain_2_2_2_max_f4\")(F4)\n",
    "            \n",
    "            # 4th layer\n",
    "            F2 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_3_2_2_conv_f2\")(F2)\n",
    "            F3 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_3_2_2_conv_f3\")(F3)\n",
    "            F4 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_3_2_2_conv_f4\")(F4)\n",
    "            F5 = KL.Conv2D(128, (2,2), padding=\"same\", name=\"domain_3_2_2_conv_f5\")(F5)\n",
    "            \n",
    "            F2 = KL.Activation('relu')(F2)\n",
    "            F3 = KL.Activation('relu')(F3)\n",
    "            F4 = KL.Activation('relu')(F4)\n",
    "            F5 = KL.Activation('relu')(F5)\n",
    "            \n",
    "            F2 = KL.MaxPool2D((2,2), name=\"domain_3_2_2_max_f2\")(F2)\n",
    "            F3 = KL.MaxPool2D((2,2), name=\"domain_3_2_2_max_f3\")(F3)\n",
    "            F4 = KL.MaxPool2D((2,2), name=\"domain_3_2_2_max_f4\")(F4)\n",
    "            F5 = KL.MaxPool2D((2,2), name=\"domain_3_2_2_max_f5\")(F5)\n",
    "            \n",
    "            \n",
    "            # All 1*1 layer\n",
    "            F2 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_all_1_1_conv_f2\")(F2)\n",
    "            F3 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_all_1_1_conv_f3\")(F3)\n",
    "            F4 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_all_1_1_conv_f4\")(F4)\n",
    "            F5 = KL.Conv2D(128, (1,1), padding=\"same\", name=\"domain_all_1_1_conv_f5\")(F5)\n",
    "            \n",
    "            F2 = KL.Activation('relu')(F2)\n",
    "            F3 = KL.Activation('relu')(F3)\n",
    "            F4 = KL.Activation('relu')(F4)\n",
    "            F5 = KL.Activation('relu')(F5)\n",
    "            \n",
    "            F2 = KL.Flatten()(F2)\n",
    "            F3 = KL.Flatten()(F3)\n",
    "            F4 = KL.Flatten()(F4)\n",
    "            F5 = KL.Flatten()(F5)\n",
    "\n",
    "            # Dense layer\n",
    "            F2 = KL.Dense(128, activation='relu', name=\"domain_dense_f2\")(F2)\n",
    "            F3 = KL.Dense(128, activation='relu', name=\"domain_dense_f3\")(F3)\n",
    "            F4 = KL.Dense(128, activation='relu', name=\"domain_dense_f4\")(F4)\n",
    "            F5 = KL.Dense(128, activation='relu', name=\"domain_dense_f5\")(F5)\n",
    "            \n",
    "            # Concatenate\n",
    "            F_out = KL.Concatenate()([F2, F3, F4, F5])\n",
    "            \n",
    "            # Fully connected layer\n",
    "            F_out = KL.Dense(512, activation='relu', name=\"domain_output_fc1\")(F_out)\n",
    "            F_out = KL.Dense(128, activation='relu', name=\"domain_output_fc2\")(F_out)\n",
    "            \n",
    "            # Output Layer\n",
    "            domain_class_logits = KL.Dense(2, activation='softmax', name=\"domain_output_logits\")(F_out)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # During the training, the batch size = 2, we need to split the feature map\n",
    "            # so that the COCO data feature map goes into the maskrcnn part\n",
    "            \n",
    "            \n",
    "            if mode == \"training\":\n",
    "\n",
    "                P2 = KL.Lambda(layer_slice)(P2)\n",
    "                P3 = KL.Lambda(layer_slice)(P3)\n",
    "                P4 = KL.Lambda(layer_slice)(P4)\n",
    "                P5 = KL.Lambda(layer_slice)(P5)\n",
    "                P6 = KL.Lambda(layer_slice)(P6)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "            mrcnn_feature_maps = [P2, P3, P4, P5]\n",
    "\n",
    "            # Generate Anchors\n",
    "            self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                                          config.RPN_ANCHOR_RATIOS,\n",
    "                                                          config.BACKBONE_SHAPES,\n",
    "                                                          config.BACKBONE_STRIDES,\n",
    "                                                          config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "            # RPN Model\n",
    "            rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\n",
    "                                  len(config.RPN_ANCHOR_RATIOS), 256)\n",
    "            # Loop through pyramid layers\n",
    "            layer_outputs = []  # list of lists\n",
    "            for p in rpn_feature_maps:\n",
    "                layer_outputs.append(rpn([p]))\n",
    "            # Concatenate layer outputs\n",
    "            # Convert from list of lists of level outputs to list of lists\n",
    "            # of outputs across levels.\n",
    "            # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "            output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
    "            outputs = list(zip(*layer_outputs))\n",
    "            outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n",
    "                       for o, n in zip(outputs, output_names)]\n",
    "\n",
    "            rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "            # Generate proposals\n",
    "            # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "            # and zero padded.\n",
    "            proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n",
    "                else config.POST_NMS_ROIS_INFERENCE\n",
    "            rpn_rois = ProposalLayer(proposal_count=proposal_count,\n",
    "                                     nms_threshold=config.RPN_NMS_THRESHOLD,\n",
    "                                     name=\"ROI\",\n",
    "                                     anchors=self.anchors,\n",
    "                                     config=config)([rpn_class, rpn_bbox])\n",
    "            \n",
    "            \n",
    "            if mode == \"training\":\n",
    "                # Class ID mask to mark class IDs supported by the dataset the image\n",
    "                # came from.\n",
    "                \n",
    "                # we only parse partial image meta here\n",
    "                _, _, _, active_class_ids = KL.Lambda(lambda x: parse_image_meta_graph(x),\n",
    "                                                      mask=[None, None, None, None])(input_image_meta)\n",
    "\n",
    "                if not config.USE_RPN_ROIS:\n",
    "                    # Ignore predicted ROIs and use ROIs provided as an input.\n",
    "                    input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n",
    "                                          name=\"input_roi\", dtype=np.int32)\n",
    "                    # Normalize coordinates to 0-1 range.\n",
    "                    target_rois = KL.Lambda(lambda x: K.cast(\n",
    "                        x, tf.float32) / image_scale[:4])(input_rois)\n",
    "                else:\n",
    "                    target_rois = rpn_rois\n",
    "\n",
    "                # Generate detection targets\n",
    "                # Subsamples proposals and generates target outputs for training\n",
    "                # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "                # padded. Equally, returned rois and targets are zero padded.\n",
    "                rois, target_class_ids, target_bbox, target_mask =\\\n",
    "                    DetectionTargetLayer(config, name=\"proposal_targets\")([\n",
    "                        target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\n",
    "\n",
    "                # Network Heads\n",
    "                # TODO: verify that this handles zero padded ROIs\n",
    "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                    fpn_classifier_graph(rois, mrcnn_feature_maps, config.IMAGE_SHAPE,\n",
    "                                         config.POOL_SIZE, config.NUM_CLASSES)\n",
    "\n",
    "                mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,\n",
    "                                                  config.IMAGE_SHAPE,\n",
    "                                                  config.MASK_POOL_SIZE,\n",
    "                                                  config.NUM_CLASSES)\n",
    "\n",
    "                # TODO: clean up (use tf.identify if necessary)\n",
    "                output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n",
    "\n",
    "                # Losses\n",
    "                rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n",
    "                    [input_rpn_match, rpn_class_logits])\n",
    "                rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\n",
    "                    [input_rpn_bbox, input_rpn_match, rpn_bbox])\n",
    "                class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\n",
    "                    [target_class_ids, mrcnn_class_logits, active_class_ids])\n",
    "                bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\n",
    "                    [target_bbox, target_class_ids, mrcnn_bbox])\n",
    "                mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\n",
    "                    [target_mask, target_class_ids, mrcnn_mask])\n",
    "                \n",
    "                #domain classification loss\n",
    "                domain_loss = KL.Lambda(lambda x: domain_loss_graph(*x), name=\"domain_classification_loss\")(\n",
    "                    [input_domain_label, domain_class_logits])\n",
    "\n",
    "                # Model\n",
    "                inputs = [input_image, ori_input_image_meta,\n",
    "                          ori_input_rpn_match, ori_input_rpn_bbox, ori_input_gt_class_ids, \n",
    "                          ori_input_gt_boxes, ori_input_gt_masks, input_domain_label]\n",
    "                if not config.USE_RPN_ROIS:\n",
    "                    inputs.append(inputa_rois)\n",
    "                outputs = [rpn_class_logits, rpn_class, rpn_bbox,\n",
    "                           mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\n",
    "                           rpn_rois, output_rois,\n",
    "                           rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss, domain_loss]\n",
    "                model = KM.Model(inputs, outputs, name='mask_rcnn')\n",
    "            else:\n",
    "                # Network Heads\n",
    "                # Proposal classifier and BBox regressor heads\n",
    "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                    fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, config.IMAGE_SHAPE,\n",
    "                                         config.POOL_SIZE, config.NUM_CLASSES)\n",
    "\n",
    "                # Detections\n",
    "                # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
    "                detections = DetectionLayer(config, name=\"mrcnn_detection\")(\n",
    "                    [rpn_rois, mrcnn_class, mrcnn_bbox, ori_input_image_meta])\n",
    "\n",
    "                # Convert boxes to normalized coordinates\n",
    "                # TODO: let DetectionLayer return normalized coordinates to avoid\n",
    "                #       unnecessary conversions\n",
    "                h, w = config.IMAGE_SHAPE[:2]\n",
    "                detection_boxes = KL.Lambda(\n",
    "                    lambda x: x[..., :4] / np.array([h, w, h, w]))(detections)\n",
    "\n",
    "                # Create masks for detections\n",
    "                mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\n",
    "                                                  config.IMAGE_SHAPE,\n",
    "                                                  config.MASK_POOL_SIZE,\n",
    "                                                  config.NUM_CLASSES)\n",
    "\n",
    "                model = KM.Model([input_image, ori_input_image_meta],\n",
    "                                 [detections, mrcnn_class, mrcnn_bbox,\n",
    "                                     mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\n",
    "                                 name='mask_rcnn')\n",
    "\n",
    "            # Add multi-GPU support.\n",
    "            if config.GPU_COUNT > 1:\n",
    "                from parallel_model import ParallelModel\n",
    "                model = ParallelModel(model, config.GPU_COUNT)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def find_last(self):\n",
    "        \"\"\"Finds the last checkpoint file of the last trained model in the\n",
    "        model directory.\n",
    "        Returns:\n",
    "            log_dir: The directory where events and weights are saved\n",
    "            checkpoint_path: the path to the last checkpoint file\n",
    "        \"\"\"\n",
    "        # Get directory names. Each directory corresponds to a model\n",
    "        dir_names = next(os.walk(self.model_dir))[1]\n",
    "        key = self.config.NAME.lower()\n",
    "        dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
    "        dir_names = sorted(dir_names)\n",
    "        if not dir_names:\n",
    "            return None, None\n",
    "        # Pick last directory\n",
    "        dir_name = os.path.join(self.model_dir, dir_names[-1])\n",
    "        # Find the last checkpoint\n",
    "        checkpoints = next(os.walk(dir_name))[2]\n",
    "        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
    "        checkpoints = sorted(checkpoints)\n",
    "        if not checkpoints:\n",
    "            return dir_name, None\n",
    "        checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
    "        return dir_name, checkpoint\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, exclude=None):\n",
    "        \"\"\"Modified version of the correspoding Keras function with\n",
    "        the addition of multi-GPU support and the ability to exclude\n",
    "        some layers from loading.\n",
    "        exlude: list of layer names to excluce\n",
    "        \"\"\"\n",
    "        import h5py\n",
    "        from keras.engine import topology\n",
    "\n",
    "        if exclude:\n",
    "            by_name = True\n",
    "\n",
    "        if h5py is None:\n",
    "            raise ImportError('`load_weights` requires h5py.')\n",
    "        f = h5py.File(filepath, mode='r')\n",
    "        if 'layer_names' not in f.attrs and 'model_weights' in f:\n",
    "            f = f['model_weights']\n",
    "\n",
    "        # In multi-GPU training, we wrap the model. Get layers\n",
    "        # of the inner model because they have the weights.\n",
    "        keras_model = self.keras_model\n",
    "        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n",
    "            else keras_model.layers\n",
    "\n",
    "        # Exclude some layers\n",
    "        if exclude:\n",
    "            layers = filter(lambda l: l.name not in exclude, layers)\n",
    "\n",
    "        if by_name:\n",
    "            topology.load_weights_from_hdf5_group_by_name(f, layers)\n",
    "        else:\n",
    "            topology.load_weights_from_hdf5_group(f, layers)\n",
    "        if hasattr(f, 'close'):\n",
    "            f.close()\n",
    "\n",
    "        # Update the log directory\n",
    "        self.set_log_dir(filepath)\n",
    "\n",
    "    def get_imagenet_weights(self):\n",
    "        \"\"\"Downloads ImageNet trained weights from Keras.\n",
    "        Returns path to weights file.\n",
    "        \"\"\"\n",
    "        from keras.utils.data_utils import get_file\n",
    "        TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\\n",
    "                                 'releases/download/v0.2/'\\\n",
    "                                 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        return weights_path\n",
    "\n",
    "    def compile(self, learning_rate, momentum):\n",
    "        \"\"\"Gets the model ready for training. Adds losses, regularization, and\n",
    "        metrics. Then calls the Keras compile() function.\n",
    "        \"\"\"\n",
    "        # Optimizer object\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum,\n",
    "                                         clipnorm=5.0)\n",
    "        # Add Losses\n",
    "        # First, clear previously set losses to avoid duplication\n",
    "        self.keras_model._losses = []\n",
    "        self.keras_model._per_input_losses = {}\n",
    "        loss_names = [\"rpn_class_loss\", \"rpn_bbox_loss\",\n",
    "                      \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\", \"domain_classification_loss\"]\n",
    "        for name in loss_names:\n",
    "            layer = self.keras_model.get_layer(name)\n",
    "            if layer.output in self.keras_model.losses:\n",
    "                continue\n",
    "            self.keras_model.add_loss(\n",
    "                tf.reduce_mean(layer.output, keep_dims=True))\n",
    "\n",
    "        # Add L2 Regularization\n",
    "        # Skip gamma and beta weights of batch normalization layers.\n",
    "        reg_losses = [keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\n",
    "                      for w in self.keras_model.trainable_weights\n",
    "                      if 'gamma' not in w.name and 'beta' not in w.name]\n",
    "        self.keras_model.add_loss(tf.add_n(reg_losses))\n",
    "\n",
    "        # Compile\n",
    "        self.keras_model.compile(optimizer=optimizer, loss=[\n",
    "                                 None] * len(self.keras_model.outputs))\n",
    "\n",
    "        # Add metrics for losses\n",
    "        for name in loss_names:\n",
    "            if name in self.keras_model.metrics_names:\n",
    "                continue\n",
    "            layer = self.keras_model.get_layer(name)\n",
    "            self.keras_model.metrics_names.append(name)\n",
    "            self.keras_model.metrics_tensors.append(tf.reduce_mean(\n",
    "                layer.output, keep_dims=True))\n",
    "\n",
    "    def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):\n",
    "        \"\"\"Sets model layers as trainable if their names match\n",
    "        the given regular expression.\n",
    "        \"\"\"\n",
    "        # Print message on the first call (but not on recursive calls)\n",
    "        if verbose > 0 and keras_model is None:\n",
    "            log(\"Selecting layers to train\")\n",
    "\n",
    "        keras_model = keras_model or self.keras_model\n",
    "\n",
    "        # In multi-GPU training, we wrap the model. Get layers\n",
    "        # of the inner model because they have the weights.\n",
    "        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n",
    "            else keras_model.layers\n",
    "\n",
    "        for layer in layers:\n",
    "            # Is the layer a model?\n",
    "            if layer.__class__.__name__ == 'Model':\n",
    "                print(\"In model: \", layer.name)\n",
    "                self.set_trainable(\n",
    "                    layer_regex, keras_model=layer, indent=indent + 4)\n",
    "                continue\n",
    "\n",
    "            if not layer.weights:\n",
    "                continue\n",
    "            # Is it trainable?\n",
    "\n",
    "            def fullmatch(regex, string, flags=0):\n",
    "                \"\"\"Emulate python-3.4 re.fullmatch().\"\"\"\n",
    "                return re.match(\"(?:\" + regex + r\")\\Z\", string, flags=flags)\n",
    "\n",
    "            trainable = bool(fullmatch(layer_regex, layer.name))\n",
    "            # Update layer. If layer is a container, update inner layer.\n",
    "            if layer.__class__.__name__ == 'TimeDistributed':\n",
    "                layer.layer.trainable = trainable\n",
    "            else:\n",
    "                layer.trainable = trainable\n",
    "            # Print trainble layer names\n",
    "            if trainable and verbose > 0:\n",
    "                log(\"{}{:20}   ({})\".format(\" \" * indent, layer.name,\n",
    "                                            layer.__class__.__name__))\n",
    "\n",
    "    def set_log_dir(self, model_path=None):\n",
    "        \"\"\"Sets the model log directory and epoch counter.\n",
    "\n",
    "        model_path: If None, or a format different from what this code uses\n",
    "            then set a new log directory and start epochs from 0. Otherwise,\n",
    "            extract the log directory and the epoch counter from the file\n",
    "            name.\n",
    "        \"\"\"\n",
    "        # Set date and epoch counter as if starting a new model\n",
    "        self.epoch = 0\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        # If we have a model path with date and epochs use them\n",
    "        if model_path:\n",
    "            # Continue from we left of. Get epoch and date from the file name\n",
    "            # A sample model path might look like:\n",
    "            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5\n",
    "            regex = r\".*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.h5\"\n",
    "            m = re.match(regex, model_path)\n",
    "            if m:\n",
    "                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n",
    "                                        int(m.group(4)), int(m.group(5)))\n",
    "                self.epoch = int(m.group(6)) + 1\n",
    "\n",
    "        # Directory for training logs\n",
    "        self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\n",
    "            self.config.NAME.lower(), now))\n",
    "\n",
    "        # Path to save after each epoch. Include placeholders that get filled by Keras.\n",
    "        self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.h5\".format(\n",
    "            self.config.NAME.lower()))\n",
    "        self.checkpoint_path = self.checkpoint_path.replace(\n",
    "            \"*epoch*\", \"{epoch:04d}\")\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, domain_image_dir, domain_image_names, learning_rate, epochs, layers):\n",
    "        \"\"\"Train the model.\n",
    "        train_dataset, val_dataset: Training and validation Dataset objects.\n",
    "        learning_rate: The learning rate to train with\n",
    "        epochs: Number of training epochs. Note that previous training epochs\n",
    "                are considered to be done alreay, so this actually determines\n",
    "                the epochs to train in total rather than in this particaular\n",
    "                call.\n",
    "        layers: Allows selecting wich layers to train. It can be:\n",
    "            - A regular expression to match layer names to train\n",
    "            - One of these predefined values:\n",
    "              heaads: The RPN, classifier and mask heads of the network\n",
    "              all: All the layers\n",
    "              3+: Train Resnet stage 3 and up\n",
    "              4+: Train Resnet stage 4 and up\n",
    "              5+: Train Resnet stage 5 and up\n",
    "        \"\"\"\n",
    "        assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "\n",
    "        # Pre-defined layer regular expressions\n",
    "        layer_regex = {\n",
    "            # all layers but the backbone\n",
    "            \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            # From a specific Resnet stage and up\n",
    "            \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            # classification layers\n",
    "            \"domain\": r\"(domain\\_.*)\",\n",
    "            # All layers\n",
    "            \"all\": \".*\",\n",
    "        }\n",
    "        if layers in layer_regex.keys():\n",
    "            layers = layer_regex[layers]\n",
    "        \n",
    "        # Data generators\n",
    "        train_generator = domain_data_generator(train_dataset, self.config,  domain_image_dir, domain_image_names, shuffle=True,\n",
    "                                         batch_size=2)\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.TensorBoard(log_dir=self.log_dir,\n",
    "                                        histogram_freq=0, write_graph=True, write_images=False),\n",
    "            keras.callbacks.ModelCheckpoint(self.checkpoint_path,\n",
    "                                            verbose=0, save_weights_only=True),\n",
    "        ]\n",
    "\n",
    "        # Train\n",
    "        log(\"\\nStarting at epoch {}. LR={}\\n\".format(self.epoch, learning_rate))\n",
    "        log(\"Checkpoint Path: {}\".format(self.checkpoint_path))\n",
    "        self.set_trainable(layers)\n",
    "        self.compile(learning_rate, self.config.LEARNING_MOMENTUM)\n",
    "\n",
    "        # Work-around for Windows: Keras fails on Windows when using\n",
    "        # multiprocessing workers. See discussion here:\n",
    "        # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009\n",
    "        if os.name is 'nt':\n",
    "            workers = 0\n",
    "        else:\n",
    "            workers = max(self.config.BATCH_SIZE // 2, 2)\n",
    "\n",
    "        self.keras_model.fit_generator(\n",
    "            train_generator,\n",
    "            initial_epoch=self.epoch,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=self.config.STEPS_PER_EPOCH,\n",
    "            callbacks=callbacks,\n",
    "            max_queue_size=100,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=True,\n",
    "        )\n",
    "        self.epoch = max(self.epoch, epochs)\n",
    "\n",
    "    def mold_inputs(self, images):\n",
    "        \"\"\"Takes a list of images and modifies them to the format expected\n",
    "        as an input to the neural network.\n",
    "        images: List of image matricies [height,width,depth]. Images can have\n",
    "            different sizes.\n",
    "\n",
    "        Returns 3 Numpy matricies:\n",
    "        molded_images: [N, h, w, 3]. Images resized and normalized.\n",
    "        image_metas: [N, length of meta data]. Details about each image.\n",
    "        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n",
    "            original image (padding excluded).\n",
    "        \"\"\"\n",
    "        molded_images = []\n",
    "        image_metas = []\n",
    "        windows = []\n",
    "        for image in images:\n",
    "            # Resize image to fit the model expected size\n",
    "            # TODO: move resizing to mold_image()\n",
    "            molded_image, window, scale, padding = utils.resize_image(\n",
    "                image,\n",
    "                min_dim=self.config.IMAGE_MIN_DIM,\n",
    "                max_dim=self.config.IMAGE_MAX_DIM,\n",
    "                padding=self.config.IMAGE_PADDING)\n",
    "            molded_image = mold_image(molded_image, self.config)\n",
    "            # Build image_meta\n",
    "            image_meta = compose_image_meta(\n",
    "                0, image.shape, window,\n",
    "                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\n",
    "            # Append\n",
    "            molded_images.append(molded_image)\n",
    "            windows.append(window)\n",
    "            image_metas.append(image_meta)\n",
    "        # Pack into arrays\n",
    "        molded_images = np.stack(molded_images)\n",
    "        image_metas = np.stack(image_metas)\n",
    "        windows = np.stack(windows)\n",
    "        return molded_images, image_metas, windows\n",
    "\n",
    "    def unmold_detections(self, detections, mrcnn_mask, image_shape, window):\n",
    "        \"\"\"Reformats the detections of one image from the format of the neural\n",
    "        network output to a format suitable for use in the rest of the\n",
    "        application.\n",
    "\n",
    "        detections: [N, (y1, x1, y2, x2, class_id, score)]\n",
    "        mrcnn_mask: [N, height, width, num_classes]\n",
    "        image_shape: [height, width, depth] Original size of the image before resizing\n",
    "        window: [y1, x1, y2, x2] Box in the image where the real image is\n",
    "                excluding the padding.\n",
    "\n",
    "        Returns:\n",
    "        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n",
    "        class_ids: [N] Integer class IDs for each bounding box\n",
    "        scores: [N] Float probability scores of the class_id\n",
    "        masks: [height, width, num_instances] Instance masks\n",
    "        \"\"\"\n",
    "        # How many detections do we have?\n",
    "        # Detections array is padded with zeros. Find the first class_id == 0.\n",
    "        zero_ix = np.where(detections[:, 4] == 0)[0]\n",
    "        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n",
    "\n",
    "        # Extract boxes, class_ids, scores, and class-specific masks\n",
    "        boxes = detections[:N, :4]\n",
    "        class_ids = detections[:N, 4].astype(np.int32)\n",
    "        scores = detections[:N, 5]\n",
    "        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n",
    "\n",
    "        # Compute scale and shift to translate coordinates to image domain.\n",
    "        h_scale = image_shape[0] / (window[2] - window[0])\n",
    "        w_scale = image_shape[1] / (window[3] - window[1])\n",
    "        scale = min(h_scale, w_scale)\n",
    "        shift = window[:2]  # y, x\n",
    "        scales = np.array([scale, scale, scale, scale])\n",
    "        shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n",
    "\n",
    "        # Translate bounding boxes to image domain\n",
    "        boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n",
    "\n",
    "        # Filter out detections with zero area. Often only happens in early\n",
    "        # stages of training when the network weights are still a bit random.\n",
    "        exclude_ix = np.where(\n",
    "            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n",
    "        if exclude_ix.shape[0] > 0:\n",
    "            boxes = np.delete(boxes, exclude_ix, axis=0)\n",
    "            class_ids = np.delete(class_ids, exclude_ix, axis=0)\n",
    "            scores = np.delete(scores, exclude_ix, axis=0)\n",
    "            masks = np.delete(masks, exclude_ix, axis=0)\n",
    "            N = class_ids.shape[0]\n",
    "\n",
    "        # Resize masks to original image size and set boundary threshold.\n",
    "        full_masks = []\n",
    "        for i in range(N):\n",
    "            # Convert neural network mask to full size mask\n",
    "            full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)\n",
    "            full_masks.append(full_mask)\n",
    "        full_masks = np.stack(full_masks, axis=-1)\\\n",
    "            if full_masks else np.empty((0,) + masks.shape[1:3])\n",
    "\n",
    "        return boxes, class_ids, scores, full_masks\n",
    "\n",
    "    def detect(self, images, verbose=0):\n",
    "        \"\"\"Runs the detection pipeline.\n",
    "\n",
    "        images: List of images, potentially of different sizes.\n",
    "\n",
    "        Returns a list of dicts, one dict per image. The dict contains:\n",
    "        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "        class_ids: [N] int class IDs\n",
    "        scores: [N] float probability scores for the class IDs\n",
    "        masks: [H, W, N] instance binary masks\n",
    "        \"\"\"\n",
    "        assert self.mode == \"inference\", \"Create model in inference mode.\"\n",
    "        assert len(\n",
    "            images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\n",
    "\n",
    "        if verbose:\n",
    "            log(\"Processing {} images\".format(len(images)))\n",
    "            for image in images:\n",
    "                log(\"image\", image)\n",
    "        # Mold inputs to format expected by the neural network\n",
    "        molded_images, image_metas, windows = self.mold_inputs(images)\n",
    "        if verbose:\n",
    "            log(\"molded_images\", molded_images)\n",
    "            log(\"image_metas\", image_metas)\n",
    "        # Run object detection\n",
    "        detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, \\\n",
    "            rois, rpn_class, rpn_bbox =\\\n",
    "            self.keras_model.predict([molded_images, image_metas], verbose=0)\n",
    "        # Process detections\n",
    "        results = []\n",
    "        for i, image in enumerate(images):\n",
    "            final_rois, final_class_ids, final_scores, final_masks =\\\n",
    "                self.unmold_detections(detections[i], mrcnn_mask[i],\n",
    "                                       image.shape, windows[i])\n",
    "            results.append({\n",
    "                \"rois\": final_rois,\n",
    "                \"class_ids\": final_class_ids,\n",
    "                \"scores\": final_scores,\n",
    "                \"masks\": final_masks,\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def ancestor(self, tensor, name, checked=None):\n",
    "        \"\"\"Finds the ancestor of a TF tensor in the computation graph.\n",
    "        tensor: TensorFlow symbolic tensor.\n",
    "        name: Name of ancestor tensor to find\n",
    "        checked: For internal use. A list of tensors that were already\n",
    "                 searched to avoid loops in traversing the graph.\n",
    "        \"\"\"\n",
    "        checked = checked if checked is not None else []\n",
    "        # Put a limit on how deep we go to avoid very long loops\n",
    "        if len(checked) > 500:\n",
    "            return None\n",
    "        # Convert name to a regex and allow matching a number prefix\n",
    "        # because Keras adds them automatically\n",
    "        if isinstance(name, str):\n",
    "            name = re.compile(name.replace(\"/\", r\"(\\_\\d+)*/\"))\n",
    "\n",
    "        parents = tensor.op.inputs\n",
    "        for p in parents:\n",
    "            if p in checked:\n",
    "                continue\n",
    "            if bool(re.fullmatch(name, p.name)):\n",
    "                return p\n",
    "            checked.append(p)\n",
    "            a = self.ancestor(p, name, checked)\n",
    "            if a is not None:\n",
    "                return a\n",
    "        return None\n",
    "\n",
    "    def find_trainable_layer(self, layer):\n",
    "        \"\"\"If a layer is encapsulated by another layer, this function\n",
    "        digs through the encapsulation and returns the layer that holds\n",
    "        the weights.\n",
    "        \"\"\"\n",
    "        if layer.__class__.__name__ == 'TimeDistributed':\n",
    "            return self.find_trainable_layer(layer.layer)\n",
    "        return layer\n",
    "\n",
    "    def get_trainable_layers(self):\n",
    "        \"\"\"Returns a list of layers that have weights.\"\"\"\n",
    "        layers = []\n",
    "        # Loop through all layers\n",
    "        for l in self.keras_model.layers:\n",
    "            # If layer is a wrapper, find inner trainable layer\n",
    "            l = self.find_trainable_layer(l)\n",
    "            # Include layer if it has weights\n",
    "            if l.get_weights():\n",
    "                layers.append(l)\n",
    "        return layers\n",
    "\n",
    "    def run_graph(self, images, outputs):\n",
    "        \"\"\"Runs a sub-set of the computation graph that computes the given\n",
    "        outputs.\n",
    "\n",
    "        outputs: List of tuples (name, tensor) to compute. The tensors are\n",
    "            symbolic TensorFlow tensors and the names are for easy tracking.\n",
    "\n",
    "        Returns an ordered dict of results. Keys are the names received in the\n",
    "        input and values are Numpy arrays.\n",
    "        \"\"\"\n",
    "        model = self.keras_model\n",
    "\n",
    "        # Organize desired outputs into an ordered dict\n",
    "        outputs = OrderedDict(outputs)\n",
    "        for o in outputs.values():\n",
    "            assert o is not None\n",
    "\n",
    "        # Build a Keras function to run parts of the computation graph\n",
    "        inputs = model.inputs\n",
    "        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "            inputs += [K.learning_phase()]\n",
    "        kf = K.function(model.inputs, list(outputs.values()))\n",
    "\n",
    "        # Run inference\n",
    "        molded_images, image_metas, windows = self.mold_inputs(images)\n",
    "        # TODO: support training mode?\n",
    "        # if TEST_MODE == \"training\":\n",
    "        #     model_in = [molded_images, image_metas,\n",
    "        #                 target_rpn_match, target_rpn_bbox,\n",
    "        #                 gt_boxes, gt_masks]\n",
    "        #     if not config.USE_RPN_ROIS:\n",
    "        #         model_in.append(target_rois)\n",
    "        #     if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "        #         model_in.append(1.)\n",
    "        #     outputs_np = kf(model_in)\n",
    "        # else:\n",
    "\n",
    "        model_in = [molded_images, image_metas]\n",
    "        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "            model_in.append(0.)\n",
    "        outputs_np = kf(model_in)\n",
    "\n",
    "        # Pack the generated Numpy arrays into a a dict and log the results.\n",
    "        outputs_np = OrderedDict([(k, v)\n",
    "                                  for k, v in zip(outputs.keys(), outputs_np)])\n",
    "        for k, v in outputs_np.items():\n",
    "            log(k, v)\n",
    "        return outputs_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID (masked): 0\n"
     ]
    }
   ],
   "source": [
    "domain_model = Domain_MaskRCNN(mode=\"training\", model_dir=model_dir,\n",
    "                          config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_image (InputLayer)         (None, 1024, 1024, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 1030, 1030, 3) 0           input_image[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 512, 512, 64)  9472        zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNorm)             (None, 512, 512, 64)  256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512, 512, 64)  0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 256, 256, 64)  0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 256, 256, 64)  4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNorm)        (None, 256, 256, 64)  256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 256, 256, 64)  0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 256, 256, 64)  36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNorm)        (None, 256, 256, 64)  256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 256, 256, 64)  0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 256, 256, 256) 16640       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 256, 256, 256) 16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNorm)        (None, 256, 256, 256) 1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNorm)         (None, 256, 256, 256) 1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 256, 256, 256) 0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_out (Activation)           (None, 256, 256, 256) 0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 256, 256, 64)  16448       res2a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNorm)        (None, 256, 256, 64)  256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 256, 256, 64)  0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 256, 256, 64)  36928       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNorm)        (None, 256, 256, 64)  256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 256, 256, 64)  0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 256, 256, 256) 16640       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNorm)        (None, 256, 256, 256) 1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 256, 256, 256) 0           bn2b_branch2c[0][0]              \n",
      "                                                                   res2a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res2b_out (Activation)           (None, 256, 256, 256) 0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 256, 256, 64)  16448       res2b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNorm)        (None, 256, 256, 64)  256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 256, 256, 64)  0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 256, 256, 64)  36928       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNorm)        (None, 256, 256, 64)  256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 256, 256, 64)  0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 256, 256, 256) 16640       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNorm)        (None, 256, 256, 256) 1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 256, 256, 256) 0           bn2c_branch2c[0][0]              \n",
      "                                                                   res2b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res2c_out (Activation)           (None, 256, 256, 256) 0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 128, 128, 128) 32896       res2c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNorm)        (None, 128, 128, 128) 512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 128, 128, 128) 0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 128, 128, 128) 147584      activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNorm)        (None, 128, 128, 128) 512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 128, 128, 128) 0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 128, 128, 512) 66048       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 128, 128, 512) 131584      res2c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNorm)        (None, 128, 128, 512) 2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNorm)         (None, 128, 128, 512) 2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 128, 128, 512) 0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res3a_out (Activation)           (None, 128, 128, 512) 0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 128, 128, 128) 65664       res3a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNorm)        (None, 128, 128, 128) 512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 128, 128, 128) 0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 128, 128, 128) 147584      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNorm)        (None, 128, 128, 128) 512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 128, 128, 128) 0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 128, 128, 512) 66048       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNorm)        (None, 128, 128, 512) 2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 128, 128, 512) 0           bn3b_branch2c[0][0]              \n",
      "                                                                   res3a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res3b_out (Activation)           (None, 128, 128, 512) 0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 128, 128, 128) 65664       res3b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNorm)        (None, 128, 128, 128) 512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 128, 128, 128) 0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 128, 128, 128) 147584      activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNorm)        (None, 128, 128, 128) 512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 128, 128, 128) 0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 128, 128, 512) 66048       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNorm)        (None, 128, 128, 512) 2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 128, 128, 512) 0           bn3c_branch2c[0][0]              \n",
      "                                                                   res3b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res3c_out (Activation)           (None, 128, 128, 512) 0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 128, 128, 128) 65664       res3c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNorm)        (None, 128, 128, 128) 512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 128, 128, 128) 0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 128, 128, 128) 147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNorm)        (None, 128, 128, 128) 512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 128, 128, 128) 0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 128, 128, 512) 66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNorm)        (None, 128, 128, 512) 2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 128, 128, 512) 0           bn3d_branch2c[0][0]              \n",
      "                                                                   res3c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res3d_out (Activation)           (None, 128, 128, 512) 0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 64, 64, 256)   131328      res3d_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 64, 64, 256)   0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 64, 64, 256)   0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 64, 64, 1024)  525312      res3d_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNorm)         (None, 64, 64, 1024)  4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 64, 64, 1024)  0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res4a_out (Activation)           (None, 64, 64, 1024)  0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 64, 64, 256)   0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 64, 64, 256)   0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 64, 64, 1024)  0           bn4b_branch2c[0][0]              \n",
      "                                                                   res4a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4b_out (Activation)           (None, 64, 64, 1024)  0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 64, 64, 256)   0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 64, 64, 256)   0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 64, 64, 1024)  0           bn4c_branch2c[0][0]              \n",
      "                                                                   res4b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4c_out (Activation)           (None, 64, 64, 1024)  0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 64, 64, 256)   0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 64, 64, 256)   0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 64, 64, 1024)  0           bn4d_branch2c[0][0]              \n",
      "                                                                   res4c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4d_out (Activation)           (None, 64, 64, 1024)  0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4d_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 64, 64, 256)   0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 64, 64, 256)   0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 64, 64, 1024)  0           bn4e_branch2c[0][0]              \n",
      "                                                                   res4d_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4e_out (Activation)           (None, 64, 64, 1024)  0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4e_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 64, 64, 256)   0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 64, 64, 256)   0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 64, 64, 1024)  0           bn4f_branch2c[0][0]              \n",
      "                                                                   res4e_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4f_out (Activation)           (None, 64, 64, 1024)  0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4g_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4f_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4g_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4g_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 64, 64, 256)   0           bn4g_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4g_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4g_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4g_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 64, 64, 256)   0           bn4g_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4g_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4g_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4g_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 64, 64, 1024)  0           bn4g_branch2c[0][0]              \n",
      "                                                                   res4f_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4g_out (Activation)           (None, 64, 64, 1024)  0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4h_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4g_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4h_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4h_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 64, 64, 256)   0           bn4h_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4h_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4h_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4h_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 64, 64, 256)   0           bn4h_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4h_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4h_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4h_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 64, 64, 1024)  0           bn4h_branch2c[0][0]              \n",
      "                                                                   res4g_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4h_out (Activation)           (None, 64, 64, 1024)  0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4i_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4h_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4i_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4i_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 64, 64, 256)   0           bn4i_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4i_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4i_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4i_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 64, 64, 256)   0           bn4i_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4i_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4i_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4i_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 64, 64, 1024)  0           bn4i_branch2c[0][0]              \n",
      "                                                                   res4h_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4i_out (Activation)           (None, 64, 64, 1024)  0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4j_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4i_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4j_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4j_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 64, 64, 256)   0           bn4j_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4j_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4j_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4j_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 64, 64, 256)   0           bn4j_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4j_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4j_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4j_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_17 (Add)                     (None, 64, 64, 1024)  0           bn4j_branch2c[0][0]              \n",
      "                                                                   res4i_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4j_out (Activation)           (None, 64, 64, 1024)  0           add_17[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4k_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4j_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4k_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4k_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 64, 64, 256)   0           bn4k_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4k_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4k_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4k_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 64, 64, 256)   0           bn4k_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4k_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4k_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4k_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_18 (Add)                     (None, 64, 64, 1024)  0           bn4k_branch2c[0][0]              \n",
      "                                                                   res4j_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4k_out (Activation)           (None, 64, 64, 1024)  0           add_18[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4l_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4k_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4l_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4l_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 64, 64, 256)   0           bn4l_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4l_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4l_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4l_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 64, 64, 256)   0           bn4l_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4l_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4l_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4l_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_19 (Add)                     (None, 64, 64, 1024)  0           bn4l_branch2c[0][0]              \n",
      "                                                                   res4k_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4l_out (Activation)           (None, 64, 64, 1024)  0           add_19[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4m_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4l_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4m_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4m_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 64, 64, 256)   0           bn4m_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4m_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4m_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4m_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 64, 64, 256)   0           bn4m_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4m_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4m_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4m_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_20 (Add)                     (None, 64, 64, 1024)  0           bn4m_branch2c[0][0]              \n",
      "                                                                   res4l_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4m_out (Activation)           (None, 64, 64, 1024)  0           add_20[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4n_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4m_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4n_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4n_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 64, 64, 256)   0           bn4n_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4n_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4n_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4n_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 64, 64, 256)   0           bn4n_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4n_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4n_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4n_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_21 (Add)                     (None, 64, 64, 1024)  0           bn4n_branch2c[0][0]              \n",
      "                                                                   res4m_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4n_out (Activation)           (None, 64, 64, 1024)  0           add_21[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4o_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4n_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4o_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4o_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 64, 64, 256)   0           bn4o_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4o_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4o_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4o_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 64, 64, 256)   0           bn4o_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4o_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4o_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4o_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_22 (Add)                     (None, 64, 64, 1024)  0           bn4o_branch2c[0][0]              \n",
      "                                                                   res4n_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4o_out (Activation)           (None, 64, 64, 1024)  0           add_22[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4p_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4o_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4p_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4p_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 64, 64, 256)   0           bn4p_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4p_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4p_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4p_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 64, 64, 256)   0           bn4p_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4p_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4p_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4p_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_23 (Add)                     (None, 64, 64, 1024)  0           bn4p_branch2c[0][0]              \n",
      "                                                                   res4o_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4p_out (Activation)           (None, 64, 64, 1024)  0           add_23[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4q_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4p_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4q_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4q_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 64, 64, 256)   0           bn4q_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4q_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4q_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4q_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 64, 64, 256)   0           bn4q_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4q_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4q_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4q_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_24 (Add)                     (None, 64, 64, 1024)  0           bn4q_branch2c[0][0]              \n",
      "                                                                   res4p_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4q_out (Activation)           (None, 64, 64, 1024)  0           add_24[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4r_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4q_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4r_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4r_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, 64, 64, 256)   0           bn4r_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4r_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4r_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4r_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, 64, 64, 256)   0           bn4r_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4r_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_51[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4r_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4r_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_25 (Add)                     (None, 64, 64, 1024)  0           bn4r_branch2c[0][0]              \n",
      "                                                                   res4q_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4r_out (Activation)           (None, 64, 64, 1024)  0           add_25[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4s_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4r_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4s_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4s_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, 64, 64, 256)   0           bn4s_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4s_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4s_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4s_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, 64, 64, 256)   0           bn4s_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4s_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4s_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4s_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_26 (Add)                     (None, 64, 64, 1024)  0           bn4s_branch2c[0][0]              \n",
      "                                                                   res4r_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4s_out (Activation)           (None, 64, 64, 1024)  0           add_26[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4t_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4s_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4t_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4t_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, 64, 64, 256)   0           bn4t_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4t_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_54[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4t_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4t_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, 64, 64, 256)   0           bn4t_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4t_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4t_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4t_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_27 (Add)                     (None, 64, 64, 1024)  0           bn4t_branch2c[0][0]              \n",
      "                                                                   res4s_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4t_out (Activation)           (None, 64, 64, 1024)  0           add_27[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4u_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4t_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4u_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4u_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, 64, 64, 256)   0           bn4u_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4u_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4u_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4u_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_57 (Activation)       (None, 64, 64, 256)   0           bn4u_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4u_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_57[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4u_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4u_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_28 (Add)                     (None, 64, 64, 1024)  0           bn4u_branch2c[0][0]              \n",
      "                                                                   res4t_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4u_out (Activation)           (None, 64, 64, 1024)  0           add_28[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4v_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4u_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4v_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4v_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, 64, 64, 256)   0           bn4v_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4v_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4v_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4v_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, 64, 64, 256)   0           bn4v_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4v_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_59[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4v_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4v_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_29 (Add)                     (None, 64, 64, 1024)  0           bn4v_branch2c[0][0]              \n",
      "                                                                   res4u_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4v_out (Activation)           (None, 64, 64, 1024)  0           add_29[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4w_branch2a (Conv2D)          (None, 64, 64, 256)   262400      res4v_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn4w_branch2a (BatchNorm)        (None, 64, 64, 256)   1024        res4w_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, 64, 64, 256)   0           bn4w_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4w_branch2b (Conv2D)          (None, 64, 64, 256)   590080      activation_60[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4w_branch2b (BatchNorm)        (None, 64, 64, 256)   1024        res4w_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_61 (Activation)       (None, 64, 64, 256)   0           bn4w_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4w_branch2c (Conv2D)          (None, 64, 64, 1024)  263168      activation_61[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4w_branch2c (BatchNorm)        (None, 64, 64, 1024)  4096        res4w_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_30 (Add)                     (None, 64, 64, 1024)  0           bn4w_branch2c[0][0]              \n",
      "                                                                   res4v_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res4w_out (Activation)           (None, 64, 64, 1024)  0           add_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 32, 32, 512)   524800      res4w_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNorm)        (None, 32, 32, 512)   2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_62 (Activation)       (None, 32, 32, 512)   0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 32, 32, 512)   2359808     activation_62[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNorm)        (None, 32, 32, 512)   2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_63 (Activation)       (None, 32, 32, 512)   0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 32, 32, 2048)  1050624     activation_63[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 32, 32, 2048)  2099200     res4w_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNorm)        (None, 32, 32, 2048)  8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNorm)         (None, 32, 32, 2048)  8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_31 (Add)                     (None, 32, 32, 2048)  0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res5a_out (Activation)           (None, 32, 32, 2048)  0           add_31[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 32, 32, 512)   1049088     res5a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNorm)        (None, 32, 32, 512)   2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_64 (Activation)       (None, 32, 32, 512)   0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 32, 32, 512)   2359808     activation_64[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNorm)        (None, 32, 32, 512)   2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_65 (Activation)       (None, 32, 32, 512)   0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 32, 32, 2048)  1050624     activation_65[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNorm)        (None, 32, 32, 2048)  8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_32 (Add)                     (None, 32, 32, 2048)  0           bn5b_branch2c[0][0]              \n",
      "                                                                   res5a_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res5b_out (Activation)           (None, 32, 32, 2048)  0           add_32[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 32, 32, 512)   1049088     res5b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNorm)        (None, 32, 32, 512)   2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_66 (Activation)       (None, 32, 32, 512)   0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 32, 32, 512)   2359808     activation_66[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNorm)        (None, 32, 32, 512)   2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, 32, 32, 512)   0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 32, 32, 2048)  1050624     activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNorm)        (None, 32, 32, 2048)  8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_33 (Add)                     (None, 32, 32, 2048)  0           bn5c_branch2c[0][0]              \n",
      "                                                                   res5b_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res5c_out (Activation)           (None, 32, 32, 2048)  0           add_33[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "fpn_c5p5 (Conv2D)                (None, 32, 32, 256)   524544      res5c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p5upsampled (UpSampling2D)   (None, 64, 64, 256)   0           fpn_c5p5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fpn_c4p4 (Conv2D)                (None, 64, 64, 256)   262400      res4w_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p4add (Add)                  (None, 64, 64, 256)   0           fpn_p5upsampled[0][0]            \n",
      "                                                                   fpn_c4p4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p4upsampled (UpSampling2D)   (None, 128, 128, 256) 0           fpn_p4add[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_c3p3 (Conv2D)                (None, 128, 128, 256) 131328      res3d_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p3add (Add)                  (None, 128, 128, 256) 0           fpn_p4upsampled[0][0]            \n",
      "                                                                   fpn_c3p3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p3upsampled (UpSampling2D)   (None, 256, 256, 256) 0           fpn_p3add[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_c2p2 (Conv2D)                (None, 256, 256, 256) 65792       res2c_out[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p2add (Add)                  (None, 256, 256, 256) 0           fpn_p3upsampled[0][0]            \n",
      "                                                                   fpn_c2p2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p5 (Conv2D)                  (None, 32, 32, 256)   590080      fpn_c5p5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p2 (Conv2D)                  (None, 256, 256, 256) 590080      fpn_p2add[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p3 (Conv2D)                  (None, 128, 128, 256) 590080      fpn_p3add[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p4 (Conv2D)                  (None, 64, 64, 256)   590080      fpn_p4add[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fpn_p6 (MaxPooling2D)            (None, 16, 16, 256)   0           fpn_p5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 256, 256, 256) 0           fpn_p2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 128, 128, 256) 0           fpn_p3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 64, 64, 256)   0           fpn_p4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 32, 32, 256)   0           fpn_p5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 16, 16, 256)   0           fpn_p6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "rpn_model (Model)                [(None, None, 2), (No 1189394     lambda_2[0][0]                   \n",
      "                                                                   lambda_3[0][0]                   \n",
      "                                                                   lambda_4[0][0]                   \n",
      "                                                                   lambda_5[0][0]                   \n",
      "                                                                   lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_gt_boxes (InputLayer)      (None, None, 4)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "rpn_class (Concatenate)          (None, None, 2)       0           rpn_model[1][1]                  \n",
      "                                                                   rpn_model[2][1]                  \n",
      "                                                                   rpn_model[3][1]                  \n",
      "                                                                   rpn_model[4][1]                  \n",
      "                                                                   rpn_model[5][1]                  \n",
      "____________________________________________________________________________________________________\n",
      "rpn_bbox (Concatenate)           (None, None, 4)       0           rpn_model[1][2]                  \n",
      "                                                                   rpn_model[2][2]                  \n",
      "                                                                   rpn_model[3][2]                  \n",
      "                                                                   rpn_model[4][2]                  \n",
      "                                                                   rpn_model[5][2]                  \n",
      "____________________________________________________________________________________________________\n",
      "input_gt_class_ids (InputLayer)  (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sliced_gt_boxes (Lambda)         (None, None, 4)       0           input_gt_boxes[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "input_gt_masks (InputLayer)      (None, 56, 56, None)  0                                            \n",
      "____________________________________________________________________________________________________\n",
      "ROI (ProposalLayer)              (None, 2000, 4)       0           rpn_class[0][0]                  \n",
      "                                                                   rpn_bbox[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "sliced_gt_class_ids (Lambda)     (None, None)          0           input_gt_class_ids[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, None, 4)       0           sliced_gt_boxes[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "sliced_gt_masks (Lambda)         (None, 56, 56, None)  0           input_gt_masks[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "proposal_targets (DetectionTarge [(None, 200, 4), (Non 0           ROI[0][0]                        \n",
      "                                                                   sliced_gt_class_ids[0][0]        \n",
      "                                                                   lambda_1[0][0]                   \n",
      "                                                                   sliced_gt_masks[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "gradient_reversal_1 (GradientRev multiple              0           fpn_p2[0][0]                     \n",
      "                                                                   fpn_p3[0][0]                     \n",
      "                                                                   fpn_p4[0][0]                     \n",
      "                                                                   fpn_p5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "roi_align_mask (PyramidROIAlign) (None, 200, 14, 14, 2 0           proposal_targets[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "                                                                   lambda_3[0][0]                   \n",
      "                                                                   lambda_4[0][0]                   \n",
      "                                                                   lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "domain_0_2_2_conv_f2 (Conv2D)    (None, 255, 255, 128) 131200      gradient_reversal_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_conv1 (TimeDistribute (None, 200, 14, 14, 2 590080      roi_align_mask[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "domain_0_2_2_max_f2 (MaxPooling2 (None, 127, 127, 128) 0           domain_0_2_2_conv_f2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_0_1_1_conv_f3 (Conv2D)    (None, 128, 128, 128) 32896       gradient_reversal_1[1][0]        \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_bn1 (TimeDistributed) (None, 200, 14, 14, 2 1024        mrcnn_mask_conv1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_2_2_conv_f2 (Conv2D)    (None, 126, 126, 128) 65664       domain_0_2_2_max_f2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_2_2_conv_f3 (Conv2D)    (None, 127, 127, 128) 65664       domain_0_1_1_conv_f3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_0_1_1_conv_f4 (Conv2D)    (None, 64, 64, 128)   32896       gradient_reversal_1[2][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_71 (Activation)       (None, 200, 14, 14, 2 0           mrcnn_mask_bn1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_2_2_max_f2 (MaxPooling2 (None, 63, 63, 128)   0           domain_1_2_2_conv_f2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_2_2_max_f3 (MaxPooling2 (None, 63, 63, 128)   0           domain_1_2_2_conv_f3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_1_1_conv_f4 (Conv2D)    (None, 64, 64, 128)   16512       domain_0_1_1_conv_f4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_0_1_1_conv_f5 (Conv2D)    (None, 32, 32, 128)   32896       gradient_reversal_1[3][0]        \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_conv2 (TimeDistribute (None, 200, 14, 14, 2 590080      activation_71[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_conv_f2 (Conv2D)    (None, 62, 62, 128)   65664       domain_1_2_2_max_f2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_conv_f3 (Conv2D)    (None, 62, 62, 128)   65664       domain_1_2_2_max_f3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_conv_f4 (Conv2D)    (None, 63, 63, 128)   65664       domain_1_1_1_conv_f4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_1_1_1_conv_f5 (Conv2D)    (None, 32, 32, 128)   16512       domain_0_1_1_conv_f5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "roi_align_classifier (PyramidROI (None, 200, 7, 7, 256 0           proposal_targets[0][0]           \n",
      "                                                                   lambda_2[0][0]                   \n",
      "                                                                   lambda_3[0][0]                   \n",
      "                                                                   lambda_4[0][0]                   \n",
      "                                                                   lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_bn2 (TimeDistributed) (None, 200, 14, 14, 2 1024        mrcnn_mask_conv2[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_max_f2 (MaxPooling2 (None, 31, 31, 128)   0           domain_2_2_2_conv_f2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_max_f3 (MaxPooling2 (None, 31, 31, 128)   0           domain_2_2_2_conv_f3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_2_2_max_f4 (MaxPooling2 (None, 31, 31, 128)   0           domain_2_2_2_conv_f4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_2_1_1_conv_f5 (Conv2D)    (None, 32, 32, 128)   16512       domain_1_1_1_conv_f5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_conv1 (TimeDistribut (None, 200, 1, 1, 102 12846080    roi_align_classifier[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_72 (Activation)       (None, 200, 14, 14, 2 0           mrcnn_mask_bn2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_conv_f2 (Conv2D)    (None, 30, 30, 128)   65664       domain_2_2_2_max_f2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_conv_f3 (Conv2D)    (None, 30, 30, 128)   65664       domain_2_2_2_max_f3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_conv_f4 (Conv2D)    (None, 30, 30, 128)   65664       domain_2_2_2_max_f4[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_conv_f5 (Conv2D)    (None, 31, 31, 128)   65664       domain_2_1_1_conv_f5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_bn1 (TimeDistributed (None, 200, 1, 1, 102 4096        mrcnn_class_conv1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_conv3 (TimeDistribute (None, 200, 14, 14, 2 590080      activation_72[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_max_f2 (MaxPooling2 (None, 15, 15, 128)   0           domain_3_2_2_conv_f2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_max_f3 (MaxPooling2 (None, 15, 15, 128)   0           domain_3_2_2_conv_f3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_max_f4 (MaxPooling2 (None, 15, 15, 128)   0           domain_3_2_2_conv_f4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "domain_3_2_2_max_f5 (MaxPooling2 (None, 15, 15, 128)   0           domain_3_2_2_conv_f5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, 200, 1, 1, 102 0           mrcnn_class_bn1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_bn3 (TimeDistributed) (None, 200, 14, 14, 2 1024        mrcnn_mask_conv3[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "domain_all_1_1_conv_f2 (Conv2D)  (None, 15, 15, 128)   16512       domain_3_2_2_max_f2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_all_1_1_conv_f3 (Conv2D)  (None, 15, 15, 128)   16512       domain_3_2_2_max_f3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_all_1_1_conv_f4 (Conv2D)  (None, 15, 15, 128)   16512       domain_3_2_2_max_f4[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "domain_all_1_1_conv_f5 (Conv2D)  (None, 15, 15, 128)   16512       domain_3_2_2_max_f5[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_conv2 (TimeDistribut (None, 200, 1, 1, 102 1049600     activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, 200, 14, 14, 2 0           mrcnn_mask_bn3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 28800)         0           domain_all_1_1_conv_f2[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 28800)         0           domain_all_1_1_conv_f3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 28800)         0           domain_all_1_1_conv_f4[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 28800)         0           domain_all_1_1_conv_f5[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_bn2 (TimeDistributed (None, 200, 1, 1, 102 4096        mrcnn_class_conv2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_conv4 (TimeDistribute (None, 200, 14, 14, 2 590080      activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "domain_dense_f2 (Dense)          (None, 128)           3686528     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "domain_dense_f3 (Dense)          (None, 128)           3686528     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "domain_dense_f4 (Dense)          (None, 128)           3686528     flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "domain_dense_f5 (Dense)          (None, 128)           3686528     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, 200, 1, 1, 102 0           mrcnn_class_bn2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_bn4 (TimeDistributed) (None, 200, 14, 14, 2 1024        mrcnn_mask_conv4[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 512)           0           domain_dense_f2[0][0]            \n",
      "                                                                   domain_dense_f3[0][0]            \n",
      "                                                                   domain_dense_f4[0][0]            \n",
      "                                                                   domain_dense_f5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "pool_squeeze (Lambda)            (None, 200, 1024)     0           activation_69[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, 200, 14, 14, 2 0           mrcnn_mask_bn4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "input_image_meta (InputLayer)    (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Domain_output_fc1 (Dense)        (None, 512)           262656      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_bbox_fc (TimeDistributed)  (None, 200, 324)      332100      pool_squeeze[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_deconv (TimeDistribut (None, 200, 28, 28, 2 262400      activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "input_rpn_match (InputLayer)     (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_rpn_bbox (InputLayer)      (None, None, 4)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sliced_image_meta (Lambda)       (None, None)          0           input_image_meta[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "Domain_output_fc2 (Dense)        (None, 128)           65664       Domain_output_fc1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "rpn_class_logits (Concatenate)   (None, None, 2)       0           rpn_model[1][0]                  \n",
      "                                                                   rpn_model[2][0]                  \n",
      "                                                                   rpn_model[3][0]                  \n",
      "                                                                   rpn_model[4][0]                  \n",
      "                                                                   rpn_model[5][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_logits (TimeDistribu (None, 200, 81)       83025       pool_squeeze[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_bbox (Reshape)             (None, 200, 81, 4)    0           mrcnn_bbox_fc[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask (TimeDistributed)     (None, 200, 28, 28, 8 20817       mrcnn_mask_deconv[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "sliced_rpn_match (Lambda)        (None, None, 1)       0           input_rpn_match[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "sliced_rpn_bbox (Lambda)         (None, None, 4)       0           input_rpn_bbox[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)                [(None,), (None, None 0           sliced_image_meta[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_domain_label (InputLayer)  (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Domain_output_logits (Dense)     (None, 2)             258         Domain_output_fc2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class (TimeDistributed)    (None, 200, 81)       0           mrcnn_class_logits[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "output_rois (Lambda)             (None, 200, 4)        0           proposal_targets[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "rpn_class_loss (Lambda)          ()                    0           sliced_rpn_match[0][0]           \n",
      "                                                                   rpn_class_logits[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "rpn_bbox_loss (Lambda)           ()                    0           sliced_rpn_bbox[0][0]            \n",
      "                                                                   sliced_rpn_match[0][0]           \n",
      "                                                                   rpn_bbox[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_class_loss (Lambda)        ()                    0           proposal_targets[0][1]           \n",
      "                                                                   mrcnn_class_logits[0][0]         \n",
      "                                                                   lambda_9[0][3]                   \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_bbox_loss (Lambda)         (1, 1)                0           proposal_targets[0][2]           \n",
      "                                                                   proposal_targets[0][1]           \n",
      "                                                                   mrcnn_bbox[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "mrcnn_mask_loss (Lambda)         (1, 1)                0           proposal_targets[0][3]           \n",
      "                                                                   proposal_targets[0][1]           \n",
      "                                                                   mrcnn_mask[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "domain_classification_loss (Lamb (None,)               0           input_domain_label[0][0]         \n",
      "                                                                   Domain_output_logits[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 80,169,722\n",
      "Trainable params: 80,058,234\n",
      "Non-trainable params: 111,488\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "domain_model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "domain_image_dir = os.path.join(os.getcwd(), \"HazeTrain\")\n",
    "domain_image_names = next(os.walk(domain_image_dir))[2]\n",
    "domain_image_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = len(domain_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_idx, img_dir, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:479: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    }
   ],
   "source": [
    "x = get_image_by_id(1, domain_image_dir, domain_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape == (1024, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = get_image_by_id(28, domain_image_dir, domain_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxEH4/wEMIQsTXD1onQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[:,:,0:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,0:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9sL/D2Qh4KnHAongAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.49696350097656"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x[:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BD_Baidu_064.png'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_image_names[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = skimage.io.imread(os.path.join(domain_image_dir, domain_image_names[28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 500, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use images in COCO/train2014\n",
      "Will use annotations in COCO/annotations/instances_train2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=14.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Will use images in COCO/val2014\n",
      "Will use annotations in COCO/annotations/instances_valminusminival2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=7.76s)\n",
      "creating index...\n",
      "index created!\n",
      "Will use images in COCO/val2014\n",
      "Will use annotations in COCO/annotations/instances_minival2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Training dataset. Use the training set and 35K from the\n",
    "# validation set, as as in the Mask RCNN paper.\n",
    "year = 2014\n",
    "dataset_train = coco.CocoDataset()\n",
    "dataset_train.load_coco(COCO_DIR, \"train\", year=year, auto_download=True)\n",
    "dataset_train.load_coco(COCO_DIR, \"valminusminival\", year=year, auto_download=True)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = coco.CocoDataset()\n",
    "dataset_val.load_coco(COCO_DIR, \"minival\", year=year, auto_download=True)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model = domain_model.keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loading weights ', '/home/boyuan/Course_project/CSCE633/project/Mask_RCNN/mask_rcnn_domain.h5')\n"
     ]
    }
   ],
   "source": [
    "# Or load the last model you trained\n",
    "# weights_path = model.find_last()[1]\n",
    "# Load weights\n",
    "print(\"Loading weights \", dehaze_model_path)\n",
    "domain_model.load_weights(dehaze_model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.05132019,  0.05474571,  0.00521704, ...,  0.05630306,\n",
       "           -0.03641768,  0.01793456],\n",
       "          [ 0.00110844, -0.06778964,  0.02186352, ...,  0.0677876 ,\n",
       "           -0.00132439, -0.0612884 ],\n",
       "          [ 0.03141931, -0.07124393,  0.02933995, ...,  0.00188993,\n",
       "            0.02403351, -0.06156543],\n",
       "          ...,\n",
       "          [-0.02822889,  0.07190148,  0.00094861, ...,  0.0669722 ,\n",
       "           -0.02572981,  0.04694603],\n",
       "          [ 0.05636757,  0.02675749,  0.06260365, ...,  0.06532735,\n",
       "           -0.01837663, -0.06455874],\n",
       "          [-0.07614732, -0.06048941,  0.03576919, ..., -0.03532176,\n",
       "            0.01658802, -0.03855395]],\n",
       " \n",
       "         [[-0.0073779 ,  0.07116687,  0.04448177, ..., -0.00837007,\n",
       "            0.01424688,  0.04005597],\n",
       "          [ 0.06177517,  0.01403334, -0.00812761, ..., -0.03209098,\n",
       "            0.04197592,  0.02150483],\n",
       "          [-0.05498389,  0.00754838,  0.03439912, ...,  0.06538413,\n",
       "           -0.03121017,  0.05435117],\n",
       "          ...,\n",
       "          [ 0.06319172,  0.01583941,  0.00756714, ...,  0.03645768,\n",
       "           -0.02774028,  0.01375588],\n",
       "          [ 0.02990071, -0.02840963,  0.01995954, ..., -0.05884573,\n",
       "            0.01148993,  0.01542632],\n",
       "          [ 0.00481071,  0.06242555,  0.07106955, ...,  0.01254535,\n",
       "            0.02469123,  0.00015708]]],\n",
       " \n",
       " \n",
       "        [[[ 0.05293608, -0.01135187,  0.04894061, ..., -0.02242409,\n",
       "           -0.06497333, -0.00258121],\n",
       "          [-0.01471991, -0.07027405, -0.049368  , ..., -0.0090498 ,\n",
       "           -0.07427281,  0.01263978],\n",
       "          [ 0.00249346,  0.01962114,  0.06097782, ..., -0.0540271 ,\n",
       "            0.0506179 , -0.03111199],\n",
       "          ...,\n",
       "          [-0.03604054, -0.02599034, -0.01141622, ...,  0.06535668,\n",
       "           -0.04862626, -0.07066172],\n",
       "          [-0.07347628, -0.06873573,  0.04141121, ..., -0.02298168,\n",
       "           -0.04850455,  0.03216593],\n",
       "          [-0.02355048,  0.00477803,  0.03943158, ..., -0.07406303,\n",
       "           -0.0591168 , -0.0404676 ]],\n",
       " \n",
       "         [[-0.06489951,  0.0114877 ,  0.06411912, ..., -0.01358439,\n",
       "            0.07625266,  0.06340984],\n",
       "          [ 0.06170804,  0.01830345, -0.02531409, ..., -0.05858377,\n",
       "           -0.07440689, -0.03233609],\n",
       "          [ 0.01306877, -0.01294156, -0.06922806, ..., -0.06861314,\n",
       "            0.04182567, -0.02826845],\n",
       "          ...,\n",
       "          [ 0.01527791,  0.05632877,  0.06447893, ..., -0.04387291,\n",
       "            0.01170851,  0.01969093],\n",
       "          [ 0.01398084, -0.04765724, -0.03800832, ..., -0.02914105,\n",
       "           -0.02588513,  0.04206386],\n",
       "          [ 0.05213545, -0.03566158, -0.05706382, ..., -0.01671764,\n",
       "            0.06248314, -0.05290884]]]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_model.keras_model.get_layer('domain_2_2_2_conv_f2').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training domain classification model\n",
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/boyuan/Course_project/CSCE633/project/Mask_RCNN/logs/coco20180427T1932/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "domain_0_2_2_conv_f2   (Conv2D)\n",
      "('In model: ', 'rpn_model')\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "domain_0_1_1_conv_f3   (Conv2D)\n",
      "domain_0_1_1_conv_f4   (Conv2D)\n",
      "domain_1_2_2_conv_f2   (Conv2D)\n",
      "domain_1_2_2_conv_f3   (Conv2D)\n",
      "domain_0_1_1_conv_f5   (Conv2D)\n",
      "domain_1_1_1_conv_f4   (Conv2D)\n",
      "domain_1_1_1_conv_f5   (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "domain_2_2_2_conv_f2   (Conv2D)\n",
      "domain_2_2_2_conv_f3   (Conv2D)\n",
      "domain_2_2_2_conv_f4   (Conv2D)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "domain_2_1_1_conv_f5   (Conv2D)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "domain_3_2_2_conv_f2   (Conv2D)\n",
      "domain_3_2_2_conv_f3   (Conv2D)\n",
      "domain_3_2_2_conv_f4   (Conv2D)\n",
      "domain_3_2_2_conv_f5   (Conv2D)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "domain_all_1_1_conv_f2   (Conv2D)\n",
      "domain_all_1_1_conv_f3   (Conv2D)\n",
      "domain_all_1_1_conv_f4   (Conv2D)\n",
      "domain_all_1_1_conv_f5   (Conv2D)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "domain_dense_f2        (Dense)\n",
      "domain_dense_f3        (Dense)\n",
      "domain_dense_f4        (Dense)\n",
      "domain_dense_f5        (Dense)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "domain_output_fc1      (Dense)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "domain_output_fc2      (Dense)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "domain_output_logits   (Dense)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:479: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:479: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "/usr/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:552: RuntimeWarning: invalid value encountered in true_divide\n",
      "  zoom = (numpy.array(input.shape) - 1) / zoom_div\n",
      "/usr/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:549: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "/usr/lib/python2.7/dist-packages/scipy/ndimage/interpolation.py:552: RuntimeWarning: invalid value encountered in true_divide\n",
      "  zoom = (numpy.array(input.shape) - 1) / zoom_div\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 861s - loss: 1.3800 - rpn_class_loss: 0.0166 - rpn_bbox_loss: 0.2695 - mrcnn_class_loss: 0.2565 - mrcnn_bbox_loss: 0.1839 - mrcnn_mask_loss: 0.2660 - domain_classification_loss: 0.3876   \n"
     ]
    }
   ],
   "source": [
    "# Training - Stage 1\n",
    "print(\"Training domain classification model\")\n",
    "domain_model.train(dataset_train, dataset_val,  domain_image_dir, domain_image_names,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=1,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.USE_MINI_MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dehaze_model_path = os.path.join(root_dir, \"test_mask_rcnn_dehazing.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_model.keras_model.save_weights(save_dehaze_model_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del domain_model\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
